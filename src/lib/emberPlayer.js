import { getUserVoiceModel } from '@/lib/database';
import { textToSpeech } from '@/lib/elevenlabs';
import {
  estimateSegmentDuration,
  extractZoomScaleFromAction,
  extractFadeFromAction,
  extractPanFromAction,
  extractZoomFromAction,
  resolveMediaReference
} from '@/lib/scriptParser';

/**
 * Get voice ID with script configuration priority over story cut
 * @param {Object} segment - Segment with voice configuration
 * @param {Object} storyCut - Story cut with fallback voice IDs
 * @param {string} voiceType - 'ember' or 'narrator'
 * @returns {string|null} Voice ID or null if not found
 */
const getVoiceId = (segment, storyCut, voiceType) => {
  if (segment.voiceConfiguration && segment.voiceConfiguration[voiceType]) {
    console.log(`üé§ Using ${voiceType} voice ID from script: ${segment.voiceConfiguration[voiceType]}`);
    return segment.voiceConfiguration[voiceType];
  }

  const storyCutKey = `${voiceType}_voice_id`;
  if (storyCut[storyCutKey]) {
    console.log(`üé§ Using ${voiceType} voice ID from story cut: ${storyCut[storyCutKey]}`);
    return storyCut[storyCutKey];
  }

  return null;
};

/**
 * Audio engine for ember playback
 * Extracted from EmberDetail.jsx to improve maintainability
 */

/**
 * Handle complex contributor audio generation with preference fallbacks
 * @param {string} userPreference - User's audio preference 
 * @param {boolean} hasRecordedAudio - Whether recorded audio exists
 * @param {boolean} hasPersonalVoice - Whether personal voice model exists
 * @param {Object} audioData - Recorded audio data
 * @param {Object} userVoiceModel - User's voice model
 * @param {Object} storyCut - Story cut configuration
 * @param {string} voiceTag - Voice tag (user name)
 * @param {string} audioContent - Clean audio content
 * @param {string} content - Full content with visual actions
 * @returns {Promise<Object>} Generated audio segment
 */
const handleContributorAudioGeneration = async (userPreference, hasRecordedAudio, hasPersonalVoice, audioData, userVoiceModel, storyCut, voiceTag, audioContent, content, segment) => {
  // Decide which audio to use based on availability and preference
  if (userPreference === 'text') {
    // User wants basic text response - use narrator or ember voice for basic TTS
    console.log(`üìù ‚úÖ Using basic text response for ${voiceTag}`);

    let fallbackVoiceId = null;
    let fallbackVoiceName = null;

    // Try script configuration first, then fallback to story cut
    fallbackVoiceId = getVoiceId(segment, storyCut, 'narrator');
    if (fallbackVoiceId) {
      fallbackVoiceName = 'narrator';
    } else {
      fallbackVoiceId = getVoiceId(segment, storyCut, 'ember');
      if (fallbackVoiceId) {
        fallbackVoiceName = 'ember';
      } else {
        throw new Error(`No voice available for text response from ${voiceTag}`);
      }
    }

    // Use attribution style with fallback voice
    const narratedContent = `${voiceTag} said, "${audioContent}"`;
    const audioBlob = await textToSpeech(narratedContent, fallbackVoiceId);
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    return {
      type: 'text_response',
      audio,
      url: audioUrl,
      blob: audioBlob,
      voiceTag,
      content: narratedContent,
      fallbackVoice: fallbackVoiceName
    };
  } else if (userPreference === 'personal' && hasPersonalVoice) {
    // User wants personal voice model and it's available
    console.log(`üéôÔ∏è ‚úÖ Using personal voice model for ${voiceTag} (user preference)`);
    console.log(`üé§ Personal voice ID: ${userVoiceModel.elevenlabs_voice_id}`);

    // Use the original transcribed text if this is a recorded message, otherwise use script content
    const textToSynthesize = hasRecordedAudio && audioData.message_content
      ? audioData.message_content
      : audioContent;

    console.log(`üìù Synthesizing text: "${textToSynthesize.substring(0, 50)}..." (${hasRecordedAudio ? 'from recording' : 'from script'})`);

    const audioBlob = await textToSpeech(textToSynthesize, userVoiceModel.elevenlabs_voice_id);
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    return {
      type: 'personal_voice_model',
      audio,
      url: audioUrl,
      blob: audioBlob,
      voiceTag,
      content: textToSynthesize,
      originalContent: audioContent,
      personalVoice: userVoiceModel.elevenlabs_voice_name,
      sourceType: hasRecordedAudio ? 'transcription' : 'script'
    };
  } else if (userPreference === 'recorded' && hasRecordedAudio) {
    // User wants recorded audio and it's available
    console.log(`üéôÔ∏è ‚úÖ Using recorded audio for ${voiceTag} (user preference)`);
    console.log(`üîó Matched with recorded: "${audioData.message_content}"`);
    console.log(`üîó Audio URL:`, audioData.audio_url);

    const audio = new Audio(audioData.audio_url);
    return {
      type: 'recorded',
      audio,
      url: audioData.audio_url,
      voiceTag,
      content
    };
  } else if (userPreference === 'recorded' && !hasRecordedAudio) {
    // User wants "recorded" but no recorded audio exists - fallback to text response
    console.log(`‚ö†Ô∏è User wanted recorded audio but none available for ${voiceTag} - falling back to text response`);

    let fallbackVoiceId = null;
    let fallbackVoiceName = null;

    // For TEXT RESPONSE fallback, ALWAYS use narrator/ember voice, never contributor's personal voice
    fallbackVoiceId = getVoiceId(segment, storyCut, 'narrator');
    if (fallbackVoiceId) {
      fallbackVoiceName = 'narrator';
    } else {
      fallbackVoiceId = getVoiceId(segment, storyCut, 'ember');
      if (fallbackVoiceId) {
        fallbackVoiceName = 'ember';
      } else {
        throw new Error(`No voice available for fallback attribution from ${voiceTag}`);
      }
    }

    const narratedContent = `${voiceTag} said, "${content}"`;
    const audioBlob = await textToSpeech(narratedContent, fallbackVoiceId);
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    return {
      type: 'attribution_fallback',
      audio,
      url: audioUrl,
      blob: audioBlob,
      voiceTag,
      content: narratedContent,
      fallbackVoice: fallbackVoiceName
    };
  } else if (userPreference === 'personal' && !hasPersonalVoice) {
    // User wants personal voice but none available - fallback to text response
    console.log(`‚ö†Ô∏è User wanted personal voice but none available for ${voiceTag} - falling back to text response`);
    console.log(`‚ö†Ô∏è Voice model check failed:`, {
      userVoiceModel,
      hasPersonalVoice,
      elevenlabs_voice_id: userVoiceModel?.elevenlabs_voice_id
    });

    let fallbackVoiceId = null;
    let fallbackVoiceName = null;

    if (storyCut.narrator_voice_id) {
      fallbackVoiceId = storyCut.narrator_voice_id;
      fallbackVoiceName = 'narrator';
      console.log(`üé§ Using narrator voice for text response fallback`);
    } else if (storyCut.ember_voice_id) {
      fallbackVoiceId = storyCut.ember_voice_id;
      fallbackVoiceName = 'ember';
      console.log(`üé§ Using ember voice for text response fallback`);
    } else {
      throw new Error(`No voice available for text response fallback from ${voiceTag}`);
    }

    const narratedContent = `${voiceTag} said, "${content}"`;
    const audioBlob = await textToSpeech(narratedContent, fallbackVoiceId);
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    return {
      type: 'text_response_fallback',
      audio,
      url: audioUrl,
      blob: audioBlob,
      voiceTag,
      content: narratedContent,
      fallbackVoice: fallbackVoiceName
    };
  } else {
    // Final fallback - respect user preference even when falling back
    console.log(`üîÑ Using final fallback logic for ${voiceTag}, preference: ${userPreference}`);

    if (userPreference === 'text') {
      // User wants text response - ALWAYS use narrator/ember voice with attribution
      console.log(`üìù ‚úÖ Using text response (final fallback, user preference)`);

      let fallbackVoiceId = null;
      let fallbackVoiceName = null;

      if (storyCut.narrator_voice_id) {
        fallbackVoiceId = storyCut.narrator_voice_id;
        fallbackVoiceName = 'narrator';
      } else if (storyCut.ember_voice_id) {
        fallbackVoiceId = storyCut.ember_voice_id;
        fallbackVoiceName = 'ember';
      } else {
        throw new Error(`No voice available for text response fallback from ${voiceTag}`);
      }

      const narratedContent = `${voiceTag} said, "${content}"`;
      const audioBlob = await textToSpeech(narratedContent, fallbackVoiceId);
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);

      return {
        type: 'text_response',
        audio,
        url: audioUrl,
        blob: audioBlob,
        voiceTag,
        content: narratedContent,
        fallbackVoice: fallbackVoiceName
      };
    } else if (userPreference === 'recorded' && hasRecordedAudio) {
      console.log(`üéôÔ∏è ‚úÖ Using recorded audio (final fallback)`);
      const audio = new Audio(audioData.audio_url);
      return {
        type: 'recorded',
        audio,
        url: audioData.audio_url,
        voiceTag,
        content
      };
    } else if (userPreference === 'personal' && hasPersonalVoice) {
      console.log(`üé§ ‚úÖ Using personal voice model (final fallback)`);

      // Use the original transcribed text if this is a recorded message, otherwise use script content
      const textToSynthesize = hasRecordedAudio && audioData.message_content
        ? audioData.message_content
        : content;

      console.log(`üìù Synthesizing text (final fallback): "${textToSynthesize.substring(0, 50)}..." (${hasRecordedAudio ? 'from recording' : 'from script'})`);

      const audioBlob = await textToSpeech(textToSynthesize, userVoiceModel.elevenlabs_voice_id);
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);

      return {
        type: 'personal_voice_model',
        audio,
        url: audioUrl,
        blob: audioBlob,
        voiceTag,
        content: textToSynthesize,
        originalContent: content,
        personalVoice: userVoiceModel.elevenlabs_voice_name,
        sourceType: hasRecordedAudio ? 'transcription' : 'script'
      };
    } else {
      // Ultimate fallback when preference can't be satisfied - use narrator/ember voice
      console.log(`üìù ‚úÖ Using text response (ultimate fallback - preference not available)`);

      let fallbackVoiceId = null;
      let fallbackVoiceName = null;

      if (storyCut.narrator_voice_id) {
        fallbackVoiceId = storyCut.narrator_voice_id;
        fallbackVoiceName = 'narrator';
      } else if (storyCut.ember_voice_id) {
        fallbackVoiceId = storyCut.ember_voice_id;
        fallbackVoiceName = 'ember';
      } else {
        throw new Error(`No voice available for ultimate fallback from ${voiceTag}`);
      }

      const narratedContent = `${voiceTag} said, "${content}"`;
      const audioBlob = await textToSpeech(narratedContent, fallbackVoiceId);
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);

      return {
        type: 'ultimate_fallback',
        audio,
        url: audioUrl,
        blob: audioBlob,
        voiceTag,
        content: narratedContent,
        fallbackVoice: fallbackVoiceName
      };
    }
  }
};

/**
 * Debug helper to inspect recorded audio data and script segment matching
 * @param {Object} recordedAudio - Available recorded audio data
 * @param {Array} scriptSegments - Script segments to analyze
 */
export const debugRecordedAudio = (recordedAudio, scriptSegments) => {
  console.log('üêõ ===== RECORDED AUDIO DEBUG REPORT =====');
  console.log('üéôÔ∏è Available recorded audio:');
  Object.entries(recordedAudio).forEach(([userId, audioData]) => {
    console.log(`  üì§ User ${userId}:`);
    console.log(`    - Name: "${audioData.user_first_name}"`);
    console.log(`    - Message: "${audioData.message_content}"`);
    console.log(`    - Audio URL: ${audioData.audio_url ? '‚úÖ EXISTS' : '‚ùå MISSING'}`);
    console.log(`    - Duration: ${audioData.audio_duration_seconds}s`);
  });

  console.log('\nüìù Script segments requiring recorded audio:');
  scriptSegments.filter(seg => seg.type === 'contributor').forEach((segment, index) => {
    console.log(`  üìã Segment ${index + 1}:`);
    console.log(`    - Voice Tag: "${segment.voiceTag}"`);
    console.log(`    - Content: "${segment.content}"`);
    console.log(`    - Type: ${segment.type}`);

    // Check for matches
    const matches = Object.entries(recordedAudio).filter(([userId, audioData]) => {
      const nameMatches = audioData.user_first_name === segment.voiceTag;
      const recordedContent = audioData.message_content?.toLowerCase() || '';
      const segmentContent = segment.content.toLowerCase();
      const contentMatches = recordedContent === segmentContent ||
        recordedContent.includes(segmentContent) ||
        segmentContent.includes(recordedContent);
      return nameMatches && contentMatches;
    });

    if (matches.length > 0) {
      console.log(`    - üéôÔ∏è MATCH FOUND: User ${matches[0][0]} (${matches[0][1].user_first_name})`);
    } else {
      console.log(`    - ‚ùå NO MATCH`);
      console.log(`    - Available names: ${Object.values(recordedAudio).map(a => a.user_first_name).join(', ')}`);

      // Check for personal voice model as fallback
      const nameMatches = Object.entries(recordedAudio).filter(([userId, audioData]) => {
        return audioData.user_first_name === segment.voiceTag;
      });

      if (nameMatches.length > 0) {
        const [userId] = nameMatches[0];
        getUserVoiceModel(userId).then(voiceModel => {
          if (voiceModel && voiceModel.elevenlabs_voice_id) {
            console.log(`    - üé§ Personal Voice Model Available: ${voiceModel.elevenlabs_voice_name}`);
          } else {
            console.log(`    - üé§ No personal voice model available`);
          }
        }).catch(err => {
          console.log(`    - ‚ö†Ô∏è Error checking voice model: ${err.message}`);
        });
      }
    }
  });
  console.log('üêõ ===== END DEBUG REPORT =====');
};

/**
 * Generate audio for a single voice segment with complex preference handling
 * @param {Object} segment - Voice segment with content and metadata
 * @param {Object} storyCut - Story cut configuration with voice IDs
 * @param {Object} recordedAudio - Available recorded audio data
 * @returns {Promise<Object>} Audio segment with metadata
 */
export const generateSegmentAudio = async (segment, storyCut, recordedAudio) => {
  const { voiceTag, content, originalContent, type } = segment;

  // Use originalContent for audio synthesis, content for display/matching
  const rawAudioContent = originalContent || content;

  // Clean content for TTS: remove visual actions and voice tags
  const audioContent = rawAudioContent
    .replace(/<[^>]+>/g, '') // Remove visual actions like <COLOR:#FF0000,TRAN:0.2>
    .replace(/^\[.*?\]\s*/, '') // Remove voice tags like [NARRATOR] or [EMBER VOICE]
    .trim();

  console.log(`üéµ Generating audio for [${voiceTag}]: "${audioContent.substring(0, 50)}..."`);
  console.log(`üîç Segment type: ${type}`);
  console.log(`üé® Full content with visual actions: "${content}"`);
  console.log(`üéôÔ∏è Audio content (no visual actions): "${audioContent}"`);

  // üêõ ENHANCED DEBUG: Log all available recorded audio
  console.log('üêõ DEBUG - All recorded audio available:');
  Object.entries(recordedAudio).forEach(([userId, audioData]) => {
    console.log(`  - User ${userId} (${audioData.user_first_name}): "${audioData.message_content?.substring(0, 50)}..."`);
    console.log(`    Audio URL: ${audioData.audio_url ? 'EXISTS' : 'MISSING'}`);
  });

  try {
    if (type === 'contributor') {
      console.log(`üêõ DEBUG - Looking for recorded audio for contributor: ${voiceTag}`);
      console.log(`üêõ DEBUG - Script content: "${audioContent}"`);

      // Check per-message preference for this content
      let userPreference = 'recorded'; // default
      let foundStudioPreference = false;

      // PRIORITY 1: Check if preference is embedded in script segment
      if (segment.preference) {
        console.log(`üéØ ‚úÖ Found embedded preference in script: ${segment.preference}`);
        // Convert script preference format to audio generation format
        if (segment.preference === 'synth') {
          userPreference = 'personal';
        } else if (segment.preference === 'recorded') {
          userPreference = 'recorded';
        } else if (segment.preference === 'text') {
          userPreference = 'text';
        }
        foundStudioPreference = true;
        console.log(`üéØ ‚úÖ Using embedded script preference: ${segment.preference} ‚Üí ${userPreference}`);
      }

      // PRIORITY 2: Check StoryCutStudio contributor preferences (fallback if no embedded preference)
      if (!foundStudioPreference && window.contributorAudioPreferences) {
        console.log(`üé≠ Checking StoryCutStudio contributor preferences for ${voiceTag}`);
        console.log(`üé≠ Current audio content: "${audioContent}"`);

        // Try to find block-specific preference by matching content
        const blockSpecificKeys = Object.keys(window.contributorAudioPreferences).filter(key =>
          key.startsWith(voiceTag + '-')
        );

        console.log(`üéØ Found ${blockSpecificKeys.length} block-specific preferences for ${voiceTag}:`, blockSpecificKeys);

        let studioPreference = null;
        let selectedBlockKey = null;

        // If we have block-specific preferences, try to match by content
        if (blockSpecificKeys.length > 0) {
          // Strategy: Match the content with the original story messages to determine which block this is
          console.log(`üéØ Trying to match content "${audioContent}" with block preferences`);

          // Check if this content matches "This was at the Home Depot!" (the text response)
          const isHomeDepotContent = audioContent.toLowerCase().includes('home depot');
          const isWhatItWasContent = audioContent.toLowerCase().includes('no idea what it was');

          console.log(`üéØ Content analysis: isHomeDepot=${isHomeDepotContent}, isWhatItWas=${isWhatItWasContent}`);

          // Find the right block based on content
          for (const blockKey of blockSpecificKeys) {
            const preference = window.contributorAudioPreferences[blockKey];
            console.log(`üéØ Checking block preference: ${blockKey} ‚Üí ${preference}`);

            // If this is the Home Depot content, and there's a text/synth preference, use it
            if (isHomeDepotContent && (preference === 'text' || preference === 'synth')) {
              studioPreference = preference;
              selectedBlockKey = blockKey;
              console.log(`üéØ ‚úÖ Matched Home Depot content with ${blockKey} ‚Üí ${preference}`);
              break;
            }
            // If this is the "no idea what it was" content, and there's a recorded preference, use it
            else if (isWhatItWasContent && preference === 'recorded') {
              studioPreference = preference;
              selectedBlockKey = blockKey;
              console.log(`üéØ ‚úÖ Matched "what it was" content with ${blockKey} ‚Üí ${preference}`);
              break;
            }
          }

          // If no content-based match, use the first block preference
          if (!studioPreference && blockSpecificKeys.length > 0) {
            selectedBlockKey = blockSpecificKeys[0];
            studioPreference = window.contributorAudioPreferences[selectedBlockKey];
            console.log(`üéØ üìù Using first block preference: ${selectedBlockKey} ‚Üí ${studioPreference}`);
          }
        } else {
          // Fallback to voice tag preference if no block-specific ones exist
          studioPreference = window.contributorAudioPreferences[voiceTag];
          console.log(`üéØ üìù Using voice tag preference: ${voiceTag} ‚Üí ${studioPreference}`);
        }

        if (studioPreference) {
          foundStudioPreference = true;
          // Convert studio preference format to audio generation format
          if (studioPreference === 'synth') {
            userPreference = 'personal';
          } else if (studioPreference === 'recorded') {
            userPreference = 'recorded';
          } else if (studioPreference === 'text') {
            userPreference = 'text';
          }
          console.log(`üéØ ‚úÖ Found StoryCutStudio preference for ${voiceTag} (${selectedBlockKey}): ${studioPreference} ‚Üí ${userPreference}`);
        }
      }

      // Fallback to detailed message preferences if no studio preference found
      if (!foundStudioPreference && window.messageAudioPreferences) {
        // Strategy 1: Try to find exact content match
        const matchingKey = Object.keys(window.messageAudioPreferences).find(key => {
          const keyContent = key.split('-').slice(1).join('-'); // Remove messageIndex prefix
          return keyContent === audioContent.substring(0, 50) || audioContent.includes(keyContent);
        });

        if (matchingKey) {
          userPreference = window.messageAudioPreferences[matchingKey];
          console.log(`üéØ Found preference match: "${matchingKey}" ‚Üí ${userPreference}`);
        } else {
          // Strategy 2: Try partial content matching
          const partialMatch = Object.keys(window.messageAudioPreferences).find(key => {
            const keyContent = key.split('-').slice(1).join('-');
            return keyContent.length > 10 && audioContent.includes(keyContent);
          });

          if (partialMatch) {
            userPreference = window.messageAudioPreferences[partialMatch];
            console.log(`üéØ Found partial preference match: "${partialMatch}" ‚Üí ${userPreference}`);
          } else {
            console.log(`‚ö†Ô∏è No preference match found for content: "${audioContent.substring(0, 50)}..."`);
            console.log(`‚ö†Ô∏è Available preferences:`, Object.keys(window.messageAudioPreferences));
          }
        }
      }

      console.log(`üé§ User preference for "${audioContent.substring(0, 30)}...": ${userPreference}`);
      console.log(`üîç All available preferences:`, window.messageAudioPreferences);

      // Find the user ID by matching the voice tag (first name) with recorded audio data OR story cut contributors
      let matchingUserId = Object.entries(recordedAudio).find(([userId, audioData]) => {
        return audioData.user_first_name === voiceTag;
      });

      // If not found in recorded audio, check story cut contributors
      if (!matchingUserId && storyCut.selected_contributors) {
        console.log(`üîç No recorded audio match, checking story cut contributors for: ${voiceTag}`);
        console.log(`üîç DEBUG: Story cut selected_contributors structure:`, storyCut.selected_contributors);
        console.log(`üîç DEBUG: Looking for contributor with name: "${voiceTag}"`);

        // Log each contributor for debugging
        storyCut.selected_contributors.forEach((contributor, index) => {
          console.log(`üîç DEBUG: Contributor ${index}: name="${contributor.name}", id="${contributor.id}", role="${contributor.role}"`);
        });

        const contributor = storyCut.selected_contributors.find(c => c.name === voiceTag);
        if (contributor) {
          console.log(`‚úÖ Found contributor in story cut: ${contributor.name} (ID: ${contributor.id})`);
          matchingUserId = [contributor.id, { user_first_name: contributor.name }];
        } else {
          console.log(`‚ùå Could not find contributor with name "${voiceTag}" in story cut contributors`);
        }
      }

      if (!matchingUserId) {
        console.log(`üîç Could not find user ID for voice tag: ${voiceTag}`);
        console.log(`üîÑ Using narrator/ember voice fallback for unmatched contributor: ${voiceTag}`);

        // Final fallback: Use narrator or ember voice with attribution
        let fallbackVoiceId = null;
        let fallbackVoiceName = null;

        if (storyCut.narrator_voice_id) {
          fallbackVoiceId = storyCut.narrator_voice_id;
          fallbackVoiceName = 'narrator';
          console.log(`üé§ Using narrator voice for unmatched contributor fallback`);
        } else if (storyCut.ember_voice_id) {
          fallbackVoiceId = storyCut.ember_voice_id;
          fallbackVoiceName = 'ember';
          console.log(`üé§ Using ember voice for unmatched contributor fallback`);
        } else {
          throw new Error(`No voice available for contributor fallback: ${voiceTag}`);
        }

        const narratedContent = `${voiceTag} said, "${audioContent}"`;
        const audioBlob = await textToSpeech(narratedContent, fallbackVoiceId);
        const audioUrl = URL.createObjectURL(audioBlob);
        const audio = new Audio(audioUrl);

        return {
          type: 'contributor_fallback',
          audio,
          url: audioUrl,
          blob: audioBlob,
          voiceTag,
          content: narratedContent,
          fallbackVoice: fallbackVoiceName
        };
      } else {
        const [userId, audioData] = matchingUserId;
        console.log(`üîç Found user ID for ${voiceTag}: ${userId}`);

        // Check for recorded audio match (only if we have actual recorded audio data)
        const hasRecordedAudio = (() => {
          // If audioData doesn't have message_content, it's from story cut contributors (no recorded audio)
          if (!audioData.message_content) {
            console.log(`  - No message content - this is from story cut contributors (synth voice only)`);
            return false;
          }

          const recordedContent = audioData.message_content?.toLowerCase() || '';
          const segmentContent = audioContent.toLowerCase();

          console.log(`  - Recorded content: "${recordedContent}"`);
          console.log(`  - Segment content: "${segmentContent}"`);

          const exactMatch = recordedContent === segmentContent;
          const recordedContainsSegment = recordedContent.includes(segmentContent);
          const segmentContainsRecorded = segmentContent.includes(recordedContent);

          console.log(`  - Exact match: ${exactMatch}`);
          console.log(`  - Recorded contains segment: ${recordedContainsSegment}`);
          console.log(`  - Segment contains recorded: ${segmentContainsRecorded}`);

          const contentMatches = exactMatch || recordedContainsSegment || segmentContainsRecorded;
          console.log(`  - Content match result: ${contentMatches}`);

          return contentMatches && audioData.audio_url;
        })();

        // Check for personal voice model
        let userVoiceModel = null;
        try {
          console.log(`üîç Fetching voice model for ${voiceTag} (userId: ${userId})...`);
          userVoiceModel = await getUserVoiceModel(userId);
          console.log(`‚úÖ Voice model result for ${voiceTag}:`, userVoiceModel);
        } catch (error) {
          console.log(`‚ö†Ô∏è Error fetching voice model for ${voiceTag}:`, error.message);
        }

        const hasPersonalVoice = userVoiceModel && userVoiceModel.elevenlabs_voice_id;

        console.log(`üé§ ${voiceTag} audio options:`);
        console.log(`  - Recorded audio: ${hasRecordedAudio ? '‚úÖ' : '‚ùå'}`);
        console.log(`  - Personal voice: ${hasPersonalVoice ? '‚úÖ' : '‚ùå'} ${hasPersonalVoice ? `(${userVoiceModel.elevenlabs_voice_name})` : ''}`);
        console.log(`  - User preference: ${userPreference}`);
        console.log(`  - User ID: ${userId}`);
        console.log(`  - Voice model data:`, userVoiceModel);

        // üö® DEBUG: Specific synth preference debugging
        if (userPreference === 'personal') {
          console.log(`üö® DEBUG: User wants SYNTH voice for ${voiceTag}`);
          console.log(`üö® DEBUG: Found user ID: ${userId}`);
          console.log(`üö® DEBUG: Voice model lookup result:`, userVoiceModel);
          console.log(`üö® DEBUG: Voice model keys:`, Object.keys(userVoiceModel || {}));
          console.log(`üö® DEBUG: Has elevenlabs_voice_id?`, !!userVoiceModel?.elevenlabs_voice_id);
          console.log(`üö® DEBUG: Voice ID value:`, userVoiceModel?.elevenlabs_voice_id);
          console.log(`üö® DEBUG: Voice name value:`, userVoiceModel?.elevenlabs_voice_name);
          console.log(`üö® DEBUG: Story cut selected_contributors:`, storyCut.selected_contributors);

          if (hasPersonalVoice) {
            console.log(`üö® DEBUG: ‚úÖ Personal voice available - will use direct synthesis`);
            console.log(`üö® DEBUG: Voice ID: ${userVoiceModel.elevenlabs_voice_id}`);
            console.log(`üö® DEBUG: Voice Name: ${userVoiceModel.elevenlabs_voice_name}`);
          } else {
            console.log(`üö® DEBUG: ‚ùå No personal voice - will fall back to attribution style`);
            console.log(`üö® DEBUG: Voice model result:`, userVoiceModel);
            console.log(`üö® DEBUG: Has voice ID? ${!!userVoiceModel?.elevenlabs_voice_id}`);
            console.log(`üö® DEBUG: typeof voice ID:`, typeof userVoiceModel?.elevenlabs_voice_id);
            console.log(`üö® DEBUG: Voice ID length:`, userVoiceModel?.elevenlabs_voice_id?.length);
            console.log(`üö® DEBUG: Voice ID matches expected format?`, /^[a-zA-Z0-9_-]+$/.test(userVoiceModel?.elevenlabs_voice_id || ''));
          }
        }

        // Decision logic for audio preference handling continues in next part...
        return await handleContributorAudioGeneration(userPreference, hasRecordedAudio, hasPersonalVoice, audioData, userVoiceModel, storyCut, voiceTag, audioContent, content, segment);
      }
    } else if (type === 'ember') {
      // EMBER VOICE
      console.log(`üî• Generating EMBER voice audio: "${audioContent}"`);

      const voiceId = getVoiceId(segment, storyCut, 'ember');
      if (!voiceId) {
        throw new Error('No ember voice ID configured for this story cut');
      }

      const audioBlob = await textToSpeech(audioContent, voiceId);
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);

      return {
        type: 'ember_voice',
        audio,
        url: audioUrl,
        blob: audioBlob,
        voiceTag,
        content: audioContent
      };
    } else if (type === 'narrator') {
      // NARRATOR VOICE
      console.log(`üé§ Generating NARRATOR audio: "${audioContent}"`);

      const voiceId = getVoiceId(segment, storyCut, 'narrator');
      if (!voiceId) {
        throw new Error('No narrator voice ID configured for this story cut');
      }

      const audioBlob = await textToSpeech(audioContent, voiceId);
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);

      return {
        type: 'narrator_voice',
        audio,
        url: audioUrl,
        blob: audioBlob,
        voiceTag,
        content: audioContent
      };
    } else {
      console.log(`‚ö†Ô∏è Unknown segment type: ${type}`);
      throw new Error(`Unknown segment type: ${type}`);
    }
  } catch (error) {
    console.error(`‚ùå Error generating audio for [${voiceTag}]:`, error);
    console.error(`‚ùå Segment data:`, {
      voiceTag,
      content,
      originalContent,
      type,
      audioContent
    });
    throw error;
  }
};

/**
 * Play multiple audio segments sequentially with visual effects
 * @param {Array} segments - Audio segments to play
 * @param {Object} storyCut - Story cut configuration
 * @param {Object} recordedAudio - Available recorded audio data
 * @param {Object} stateSetters - React state setters for visual effects
 * @param {Object} ember - Ember data for media resolution
 * @returns {Promise<void>}
 */
export const playMultiVoiceAudio = async (segments, storyCut, recordedAudio, stateSetters, ember) => {
  const {
    setIsGeneratingAudio,
    setIsPlaying,
    handleExitPlay,
    handlePlaybackComplete,
    setActiveAudioSegments,
    playbackStoppedRef,
    setCurrentVoiceType,
    setCurrentVoiceTransparency,
    setCurrentMediaColor,
    setCurrentZoomScale,
    setCurrentMediaImageUrl,
    // üéØ Loading screen state setters
    setCurrentLoadingState,
    setCurrentLoadingMessage,
    setCurrentLoadingIcon,
    // üéØ Visual effects state setters
    setCurrentFadeEffect,
    setCurrentPanEffect,
    setCurrentZoomEffect,
    // üéØ Sentence-by-sentence display state setters
    setCurrentDisplayText,
    setCurrentVoiceTag,
    setCurrentSentenceIndex,
    setCurrentSegmentSentences,
    setSentenceTimeouts,
    sentenceTimeouts,
    // üéØ Media timeout management
    setMediaTimeouts,
    mediaTimeouts,
    mediaTimeoutsRef
  } = stateSetters;

  console.log('üé≠ Starting multi-voice playback with', segments.length, 'segments');

  // üêõ DEBUG: Run debug helper to analyze recorded audio matching
  debugRecordedAudio(recordedAudio, segments);

  // Reset playback flag
  playbackStoppedRef.current = false;

  try {
    // Separate media/hold/loadscreen and voice segments for different processing
    const mediaSegments = segments.filter(segment => segment.type === 'media' || segment.type === 'hold' || segment.type === 'loadscreen');
    const voiceSegments = segments.filter(segment => segment.type === 'ember' || segment.type === 'narrator' || segment.type === 'contributor');
    console.log(`üé≠ Segment breakdown: ${segments.length} total ‚Üí ${mediaSegments.length} media/hold/loadscreen + ${voiceSegments.length} voice`);

    // Generate audio for voice segments only
    console.log('‚è≥ Generating audio for voice segments...');
    const audioSegments = [];
    for (let i = 0; i < voiceSegments.length; i++) {
      const segment = voiceSegments[i];
      console.log(`üîß Generating segment ${i + 1}/${voiceSegments.length}: [${segment.voiceTag}] "${segment.originalContent || segment.content}"`);

      try {
        const audioSegment = await generateSegmentAudio(segment, storyCut, recordedAudio);
        audioSegments.push(audioSegment);
        console.log(`‚úÖ Generated segment ${i + 1}: [${segment.voiceTag}]`);
      } catch (segmentError) {
        console.error(`‚ùå Failed to generate segment ${i + 1} [${segment.voiceTag}]:`, segmentError);
        console.error(`‚ùå Segment content: "${segment.originalContent || segment.content}"`);
        console.error(`‚ùå Segment type: ${segment.type}`);

        // Instead of throwing, push null to maintain index alignment
        audioSegments.push(null);
        console.warn(`‚ö†Ô∏è Skipping segment ${i + 1} [${segment.voiceTag}] due to audio generation failure`);
      }
    }

    // Create unified timeline with media effects and voice audio
    const timeline = [];
    let audioIndex = 0;

    console.log('üé¨ Building unified timeline...');
    segments.forEach((segment, index) => {
      if (segment.type === 'hold') {
        // Hold segment - blocks timeline for set duration
        const duration = parseFloat(estimateSegmentDuration(segment.content, segment.type));
        timeline.push({
          type: 'hold',
          segment,
          duration,
          index
        });
        console.log(`‚è∏Ô∏è Timeline ${index + 1}: HOLD effect for ${duration}s - ${segment.content.substring(0, 50)}...`);
      } else if (segment.type === 'loadscreen') {
        // Load screen segment - blocks timeline for set duration with loading UI
        const duration = segment.loadDuration || parseFloat(estimateSegmentDuration(segment.content, segment.type));
        timeline.push({
          type: 'loadscreen',
          segment,
          duration,
          index
        });
        console.log(`‚è≥ Timeline ${index + 1}: LOAD SCREEN for ${duration}s - "${segment.loadMessage}"`);
      } else if (segment.type === 'media') {
        // Media segment - add to timeline for sequential processing
        timeline.push({
          type: 'media',
          segment,
          index
        });
        console.log(`üì∫ Timeline ${index + 1}: MEDIA switch - ${segment.content.substring(0, 50)}...`);
      } else if (segment.type === 'ember' || segment.type === 'narrator' || segment.type === 'contributor') {
        // Voice segment - use generated audio
        if (audioIndex < audioSegments.length) {
          const audioSegment = audioSegments[audioIndex];
          if (audioSegment) {
            timeline.push({
              type: 'voice',
              segment,
              audio: audioSegment,
              index
            });
            console.log(`üé§ Timeline ${index + 1}: VOICE audio - [${segment.voiceTag}]`);
          } else {
            console.warn(`‚ö†Ô∏è Skipping timeline step ${index + 1}: No audio available for [${segment.voiceTag}]`);
          }
          audioIndex++;
        }
      } else {
        console.warn(`‚ö†Ô∏è Unknown segment type: ${segment.type} - [${segment.voiceTag}]`);
      }
    });

    console.log('‚úÖ Generated', audioSegments.length, 'audio segments');
    console.log('üé¨ Starting unified timeline playback...');
    console.log(`üé≠ Timeline has ${timeline.length} sequential steps`);

    // Store audio segments in state for cleanup
    setActiveAudioSegments(audioSegments);

    // Switch from generating to playing state - clear all loading states
    setIsGeneratingAudio(false);
    setCurrentLoadingState(false);
    setCurrentLoadingMessage('');
    setCurrentLoadingIcon('default');
    setIsPlaying(true);

    console.log('‚úÖ Audio generation complete - starting playback');

    // No background media processing - MEDIA elements are now in timeline

    let currentTimelineIndex = 0;
    // Clear any existing media timeouts before starting new playback
    mediaTimeouts.forEach(timeoutId => clearTimeout(timeoutId));
    mediaTimeoutsRef.current.forEach(timeoutId => clearTimeout(timeoutId));
    setMediaTimeouts([]);
    mediaTimeoutsRef.current = [];

    const playNextTimelineStep = () => {
      if (playbackStoppedRef.current || currentTimelineIndex >= timeline.length) {
        // Clean up any remaining media timeouts
        console.log('üßπ Cleaning up', mediaTimeouts.length, 'media timeouts');
        mediaTimeouts.forEach(timeoutId => clearTimeout(timeoutId));
        mediaTimeoutsRef.current.forEach(timeoutId => clearTimeout(timeoutId));
        setMediaTimeouts([]);
        mediaTimeoutsRef.current = [];

        // All timeline steps finished or playback was stopped
        if (!playbackStoppedRef.current) {
          console.log('üé¨ Timeline playback complete');
          // Clear any remaining visual effects
          setCurrentVoiceType(null);
          setCurrentVoiceTransparency(0.2); // Reset to default
          setCurrentMediaColor(null);
          setCurrentZoomScale({ start: 1.0, end: 1.0 }); // Reset to default
          setCurrentMediaImageUrl(null); // Reset to default ember image
          handlePlaybackComplete();
        } else {
          console.log('üõë Timeline playback stopped');
        }
        return;
      }

      const currentStep = timeline[currentTimelineIndex];
      console.log(`üé¨ Timeline step ${currentTimelineIndex + 1}/${timeline.length}: ${currentStep.type} - ${currentStep.type === 'hold' ? `${currentStep.duration}s` : `[${currentStep.segment.voiceTag}]`}`);

      if (currentStep.type === 'hold') {
        // Hold step - apply visual effect and wait for duration
        const segment = currentStep.segment;
        const duration = currentStep.duration;

        console.log(`‚è∏Ô∏è Applying hold effect for ${duration}s: ${segment.content}`);

        // Extract fade effects first (same as MEDIA blocks)
        const hasFadeEffects = segment.content.includes('FADE-');
        let fadeEffect = null;

        if (hasFadeEffects) {
          fadeEffect = extractFadeFromAction(segment.content);
          console.log(`üé¨ HOLD fade effect detected: ${fadeEffect?.type} - ${fadeEffect?.duration}s`);
        }

        // Extract color from COLOR:#RRGGBB format
        let colorValue = null;
        if (segment.content.includes('COLOR:#')) {
          const colorMatch = segment.content.match(/COLOR:#([0-9A-Fa-f]{6})/);
          if (colorMatch) {
            colorValue = '#' + colorMatch[1];
          }
        }

        // Apply visual effects based on content
        if (segment.content.includes('Z-OUT:')) {
          // Extract zoom scale values from Z-OUT command
          const zoomScale = extractZoomScaleFromAction(segment.content);
          console.log(`üé¨ Visual effect: Image zoom out - from ${zoomScale.start} to ${zoomScale.end}`);
          setCurrentVoiceType(null); // Image display, no voice overlay
          setCurrentMediaColor(null); // Clear any color overlay
          setCurrentZoomScale(zoomScale); // Apply dynamic zoom scale
        } else if (colorValue) {
          console.log(`üé¨ Visual effect: Color overlay - ${colorValue}`);
          setCurrentVoiceType(null); // Clear any voice overlay

          // üéØ NEW APPROACH: Set color through React state, effects handled by EmberPlay
          setCurrentMediaColor(colorValue);

          // Log fade effect for debugging if present
          if (fadeEffect) {
            console.log(`üé¨ HOLD fade effect will be handled by React: ${fadeEffect.type} - ${fadeEffect.duration}s`);
          }
        } else {
          // Clear any existing color overlays if no explicit color command
          setCurrentVoiceType(null);
          setCurrentMediaColor(null);
        }

        // Wait for the specified duration, then continue to next step
        const timeoutId = setTimeout(() => {
          if (!playbackStoppedRef.current) {
            currentTimelineIndex++;
            playNextTimelineStep();
          }
        }, duration * 1000);

        // Track timeout for cleanup
        setMediaTimeouts(prev => [...prev, timeoutId]);
        mediaTimeoutsRef.current.push(timeoutId);

      } else if (currentStep.type === 'loadscreen') {
        // Load screen step - display loading UI for specified duration
        const segment = currentStep.segment;
        const duration = currentStep.duration;

        console.log(`‚è≥ Load screen step: ${duration}s - "${segment.loadMessage}"`);

        // Set loading state with message and icon
        setCurrentLoadingMessage(segment.loadMessage);
        setCurrentLoadingIcon(segment.loadIcon);
        setCurrentLoadingState(true);

        // Clear any background image/color and voice
        setCurrentMediaImageUrl(null);
        setCurrentMediaColor(null);
        setCurrentVoiceType(null);

        // Wait for the specified duration, then continue to next step
        const timeoutId = setTimeout(() => {
          if (!playbackStoppedRef.current) {
            // Clear loading state
            setCurrentLoadingState(false);
            setCurrentLoadingMessage('');
            setCurrentLoadingIcon('default');

            currentTimelineIndex++;
            playNextTimelineStep();
          }
        }, duration * 1000);

        // Track timeout for cleanup
        setMediaTimeouts(prev => [...prev, timeoutId]);
        mediaTimeoutsRef.current.push(timeoutId);

      } else if (currentStep.type === 'media') {
        // Media step - switch background image and continue immediately
        const segment = currentStep.segment;

        console.log(`üì∫ Switching to media: ${segment.content.substring(0, 50)}...`);

        // Resolve the media reference to get the actual image URL
        resolveMediaReference(segment, ember.id).then(resolvedMediaUrl => {
          console.log(`üîç Resolved media URL for "${segment.mediaPath || segment.mediaName || segment.mediaId}":`, resolvedMediaUrl);

          if (resolvedMediaUrl) {
            // Extract all effects from the visual actions array
            let fadeEffect = null;
            let panEffect = null;
            let zoomEffect = null;
            let hasLegacyZoomEffects = false;

            // Process each visual action to extract effects
            if (segment.visualActions && segment.visualActions.length > 0) {
              for (const action of segment.visualActions) {
                console.log(`üîç Processing visual action: "${action}"`);

                // Extract fade effects
                if (action.includes('FADE-') && !fadeEffect) {
                  fadeEffect = extractFadeFromAction(action);
                  if (fadeEffect) {
                    console.log(`üé¨ Fade effect extracted: ${fadeEffect.type} - ${fadeEffect.duration}s`);
                  }
                }

                // Extract pan effects
                if (action.includes('PAN-') && !panEffect) {
                  panEffect = extractPanFromAction(action);
                  if (panEffect) {
                    console.log(`üé¨ Pan effect extracted: ${panEffect.direction} - ${panEffect.duration}s`);
                  }
                }

                // Extract zoom effects
                if (action.includes('ZOOM-') && !zoomEffect) {
                  zoomEffect = extractZoomFromAction(action);
                  if (zoomEffect) {
                    console.log(`üé¨ Zoom effect extracted: ${zoomEffect.type} - ${zoomEffect.duration}s`);
                  }
                }

                // Check for legacy zoom effects
                if (action.includes('Z-OUT:')) {
                  hasLegacyZoomEffects = true;
                }
              }
            }

            // Apply legacy zoom effects (Z-OUT: system)
            if (hasLegacyZoomEffects) {
              const zoomScale = extractZoomScaleFromAction(segment.content);
              console.log(`üé¨ Applying legacy zoom effect: from ${zoomScale.start} to ${zoomScale.end}`);
              setCurrentZoomScale(zoomScale);
            }

            // üéØ NEW APPROACH: Coordinate image and effects through React state coordination
            console.log(`üé¨ Setting image URL and effects through React state coordination`);

            // Set effects state first, then image URL - this allows EmberPlay to coordinate properly
            setCurrentFadeEffect(fadeEffect);
            setCurrentPanEffect(panEffect);
            setCurrentZoomEffect(zoomEffect);

            // Set image URL - EmberPlay will now handle the coordination with effects
            setCurrentMediaImageUrl(resolvedMediaUrl);

            // Log effects for debugging
            if (fadeEffect) {
              console.log(`üé¨ Fade effect set in React state: ${fadeEffect.type} - ${fadeEffect.duration}s`);
            }
            if (panEffect) {
              console.log(`üé¨ Pan effect set in React state: ${panEffect.direction} - ${panEffect.duration}s`);
            }
            if (zoomEffect) {
              console.log(`üé¨ Zoom effect set in React state: ${zoomEffect.type} - ${zoomEffect.duration}s`);
            }

            // Clear any color overlays when switching to image
            setCurrentMediaColor(null);
          } else {
            console.warn(`‚ö†Ô∏è Could not resolve media reference: ${segment.mediaPath || segment.mediaName || segment.mediaId}`);
          }

          // Continue immediately to next timeline step (media changes are instant)
          currentTimelineIndex++;
          playNextTimelineStep();
        }).catch(error => {
          console.error('‚ùå Error resolving media reference:', error);
          // Continue anyway
          currentTimelineIndex++;
          playNextTimelineStep();
        });

      } else if (currentStep.type === 'voice') {
        // Voice step - play audio
        if (!currentStep.audio || !currentStep.audio.audio) {
          console.error(`‚ùå No audio available for voice segment [${currentStep.segment.voiceTag}] - skipping`);
          currentTimelineIndex++;
          playNextTimelineStep();
          return;
        }

        const audio = currentStep.audio.audio;
        const segment = currentStep.segment;

        console.log(`‚ñ∂Ô∏è Playing voice segment: [${segment.voiceTag}]`);

        // Clear any color overlays during voice playback
        setCurrentVoiceType(null);
        setCurrentMediaColor(null);

        // üéØ Simplified Text Display: Show full segment text when voice starts
        const displayText = segment.originalContent || segment.content;
        const voiceTag = segment.voiceTag;

        console.log(`üìù Displaying full segment text: "${displayText}"`);

        // Set text display immediately when voice starts
        setCurrentDisplayText(displayText);
        setCurrentVoiceTag(voiceTag);

        // Clear any existing sentence timeouts (cleanup from old system)
        sentenceTimeouts.forEach(timeout => clearTimeout(timeout));
        setSentenceTimeouts([]);

        // Reset sentence-related states (no longer needed)
        setCurrentSentenceIndex(0);
        setCurrentSegmentSentences([]);

        // Handle audio completion
        audio.onended = () => {
          if (playbackStoppedRef.current) return; // Don't continue if playback was stopped

          console.log(`‚úÖ Completed voice segment: [${segment.voiceTag}]`);
          currentTimelineIndex++;
          playNextTimelineStep();
        };

        // Handle errors
        audio.onerror = (error) => {
          if (playbackStoppedRef.current) return; // Don't continue if playback was stopped

          console.error(`‚ùå Error playing voice segment [${segment.voiceTag}]:`, error);
          currentTimelineIndex++;
          playNextTimelineStep(); // Continue to next timeline step
        };

        // Play the audio
        audio.play().catch(error => {
          if (playbackStoppedRef.current) return; // Don't continue if playback was stopped

          console.error(`‚ùå Failed to play voice segment [${segment.voiceTag}]:`, error);
          currentTimelineIndex++;
          playNextTimelineStep(); // Continue to next timeline step
        });
      }
    };

    // Start playing the timeline
    playNextTimelineStep();

  } catch (error) {
    console.error('‚ùå Error in multi-voice playback:', error);
    console.error('‚ùå Full error details:', error.message, error.stack);

    // Clean up media timeouts on error
    console.log('üßπ Cleaning up media timeouts on error:', mediaTimeouts.length);
    mediaTimeouts.forEach(timeoutId => clearTimeout(timeoutId));
    mediaTimeoutsRef.current.forEach(timeoutId => clearTimeout(timeoutId));
    setMediaTimeouts([]);
    mediaTimeoutsRef.current = [];

    // Reset states on error
    setIsGeneratingAudio(false);
    setCurrentLoadingState(false);
    setCurrentLoadingMessage('');
    setCurrentLoadingIcon('default');
    setIsPlaying(false);
    setActiveAudioSegments([]);

    throw error;
  }
};